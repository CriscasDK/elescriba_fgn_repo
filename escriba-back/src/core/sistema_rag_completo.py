#!/usr/bin/env python3
"""
Sistema RAG Completo con Trazabilidad y Mejora Continua
Integra consultas frecuentes (vistas materializadas) con generaci√≥n LLM
"""

import os
import json
import time
import hashlib
import asyncio
import logging
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime
from dataclasses import dataclass
from enum import Enum
from decimal import Decimal

import psycopg2
import psycopg2.extras
from openai import AzureOpenAI
from dotenv import load_dotenv

# Importar Azure Search 
try:
    from .azure_search_vectorizado import AzureSearchVectorizado
except ImportError:
    try:
        from azure_search_vectorizado import AzureSearchVectorizado
    except ImportError:
        print("WARNING: Azure Search no disponible")

def convert_db_types(obj):
    """Convertir tipos de base de datos a tipos JSON-serializables"""
    if isinstance(obj, Decimal):
        return float(obj)
    elif isinstance(obj, datetime):
        return obj.isoformat()
    elif isinstance(obj, dict):
        return {k: convert_db_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_db_types(item) for item in obj]
    else:
        return obj

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/rag_system.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Cargar variables de entorno
load_dotenv('.env.gpt41')

class TipoConsulta(Enum):
    FRECUENTE = "frecuente"
    RAG = "rag"
    HIBRIDA = "hibrida"

class MetodoResolucion(Enum):
    VISTA_MATERIALIZADA = "vista_materializada"
    BUSQUEDA_SQL = "busqueda_sql"
    LLM_GENERACION = "llm_generacion"
    CACHE = "cache"

@dataclass
class ConsultaRAG:
    usuario_id: str
    pregunta: str
    sesion_id: Optional[str] = None
    ip_cliente: Optional[str] = None
    user_agent: Optional[str] = None

@dataclass
@dataclass
class RespuestaRAG:
    texto: str
    fuentes: List[Dict[str, Any]]
    confianza: float
    metodo: MetodoResolucion
    tiempo_respuesta: int
    id: Optional[int] = None
    datos_estructurados: Optional[Dict] = None
    metadatos_llm: Optional[Dict] = None

@dataclass
class FeedbackRAG:
    calificacion: int  # 1-5
    comentario: Optional[str] = None
    aspectos: Optional[Dict[str, int]] = None  # {precision: 4, relevancia: 5}
    respuesta_esperada: Optional[str] = None

class SistemaRAGTrazable:
    """Sistema RAG con trazabilidad completa y mejora continua"""
    
    def __init__(self):
        self.db_config = {
            'host': os.getenv('POSTGRES_HOST', 'localhost'),
            'port': os.getenv('POSTGRES_PORT', '5432'),
            'database': os.getenv('POSTGRES_DB', 'documentos_juridicos_gpt4'),
            'user': os.getenv('POSTGRES_USER', 'docs_user'),
            'password': os.getenv('POSTGRES_PASSWORD', 'docs_password_2024')
        }
        
        # Cliente Azure OpenAI con cliente HTTP limpio
        import httpx
        http_client = httpx.Client(
            timeout=30.0,
            headers={"User-Agent": "RAG-System/1.0"}
        )
        
        self.azure_client = AzureOpenAI(
            api_key=os.getenv('AZURE_OPENAI_API_KEY'),
            api_version=os.getenv('AZURE_OPENAI_API_VERSION', '2024-02-15-preview'),
            azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),
            http_client=http_client
        )
        
        self.deployment_name = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME', 'gpt-4o-mini')
        
        # Templates para diferentes tipos de respuesta
        self.templates = {
            'estadisticas': """
Bas√°ndome en los datos del sistema de documentos judiciales:

{datos_estructurados}

Resumen: {contexto}

Esta informaci√≥n proviene de {num_fuentes} fuente(s) de datos procesadas autom√°ticamente.
""",
            
            'busqueda_entidades': """
He encontrado la siguiente informaci√≥n sobre "{termino_busqueda}":

{resultados_busqueda}

Fuentes consultadas: {fuentes}

¬øTe gustar√≠a que profundice en alg√∫n aspecto espec√≠fico?
""",
            
            'analisis_relaciones': """
An√°lisis de relaciones para "{consulta}":

{analisis_redes}

Esta informaci√≥n se basa en el an√°lisis de co-ocurrencia de entidades en {num_documentos} documentos procesados.
""",
            
            'pregunta_compleja': """
Contexto relevante encontrado:

{contexto_sql}

An√°lisis detallado:

{respuesta_llm}

Fuentes: {fuentes_detalle}
"""
        }

    def get_db_connection(self):
        """Obtener conexi√≥n a la base de datos"""
        return psycopg2.connect(**self.db_config)

    async def procesar_consulta(self, consulta: ConsultaRAG) -> Tuple[RespuestaRAG, int]:
        """Procesar consulta RAG con trazabilidad completa"""
        start_time = time.time()
        
        try:
            # 1. Registrar consulta
            with self.get_db_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        SELECT registrar_consulta_rag(%s, %s, %s, %s)
                    """, (consulta.usuario_id, consulta.pregunta, consulta.ip_cliente, consulta.user_agent))
                    
                    consulta_id = cur.fetchone()[0]
                    logger.info(f"Consulta registrada con ID: {consulta_id}")

            # 2. Buscar en cache
            respuesta_cache = await self._buscar_cache(consulta.pregunta)
            if respuesta_cache:
                logger.info("Respuesta encontrada en cache")
                tiempo_respuesta = int((time.time() - start_time) * 1000)
                return respuesta_cache, consulta_id

            # 3. Clasificar tipo de consulta
            tipo_consulta = await self._clasificar_consulta(consulta.pregunta)
            logger.info(f"Tipo de consulta detectado: {tipo_consulta}")

            # 4. Resolver seg√∫n tipo
            if tipo_consulta == TipoConsulta.FRECUENTE:
                respuesta = await self._resolver_consulta_frecuente(consulta.pregunta)
            elif tipo_consulta == TipoConsulta.RAG:
                respuesta = await self._resolver_consulta_rag(consulta.pregunta)
            else:
                respuesta = await self._resolver_consulta_hibrida(consulta.pregunta)

            # 5. Calcular tiempo de respuesta
            tiempo_respuesta = int((time.time() - start_time) * 1000)
            respuesta.tiempo_respuesta = tiempo_respuesta

            # 6. Registrar respuesta
            with self.get_db_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        SELECT registrar_respuesta_rag(%s, %s, %s, %s, %s, %s, %s)
                    """, (
                        consulta_id, respuesta.texto, json.dumps(convert_db_types(respuesta.fuentes)),
                        respuesta.confianza, respuesta.metodo.value,
                        json.dumps(convert_db_types(respuesta.datos_estructurados)) if respuesta.datos_estructurados else None,
                        json.dumps(convert_db_types(respuesta.metadatos_llm)) if respuesta.metadatos_llm else None
                    ))
                    respuesta_id = cur.fetchone()[0]
                    respuesta.id = respuesta_id

            # 7. Actualizar m√©tricas de consulta
            with self.get_db_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        UPDATE rag_consultas 
                        SET tiempo_respuesta_ms = %s, metodo_resolucion = %s 
                        WHERE id = %s
                    """, (tiempo_respuesta, respuesta.metodo.value, consulta_id))

            # 8. Guardar en cache si es relevante
            if respuesta.confianza >= 0.8 and tipo_consulta in [TipoConsulta.FRECUENTE, TipoConsulta.HIBRIDA]:
                await self._guardar_cache(consulta.pregunta, respuesta)

            logger.info(f"Consulta procesada exitosamente en {tiempo_respuesta}ms")
            return respuesta, consulta_id

        except Exception as e:
            logger.error(f"Error procesando consulta: {str(e)}")
            tiempo_respuesta = int((time.time() - start_time) * 1000)
            
            # Respuesta de error
            respuesta_error = RespuestaRAG(
                texto=f"Lo siento, hubo un error procesando tu consulta: {str(e)}",
                fuentes=[],
                confianza=0.0,
                metodo=MetodoResolucion.LLM_GENERACION,
                tiempo_respuesta=tiempo_respuesta
            )
            
            return respuesta_error, consulta_id if 'consulta_id' in locals() else None

    async def _buscar_cache(self, pregunta: str) -> Optional[RespuestaRAG]:
        """Buscar respuesta en cache"""
        try:
            with self.get_db_connection() as conn:
                with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                    cur.execute("SELECT * FROM buscar_respuesta_cache(%s)", (pregunta,))
                    resultado = cur.fetchone()
                    
                    if resultado:
                        return RespuestaRAG(
                            texto=resultado['respuesta'],
                            fuentes=resultado['fuentes'] or [],
                            confianza=0.9,  # Cache tiene alta confianza
                            metodo=MetodoResolucion.CACHE,
                            tiempo_respuesta=0
                        )
            return None
        except Exception as e:
            logger.warning(f"Error buscando en cache: {str(e)}")
            return None

    async def _clasificar_consulta(self, pregunta: str) -> TipoConsulta:
        """Clasificar tipo de consulta usando la funci√≥n SQL"""
        # Detectar preguntas conceptuales complejas que deben ir directamente a RAG
        pregunta_lower = pregunta.lower()
        
        # Preguntas sobre genocidio, conceptos complejos, an√°lisis profundo
        conceptos_complejos = [
            'genocidio', 'genocida', 'exterminio', 'sistematico',
            'union patriotica', 'patriotica', 'up',
            'por que', 'porque', 'como', 'cuando', 'donde',
            'analisis', 'explicacion', 'razon', 'motivo',
            'contexto', 'trasfondo', 'antecedente',
            # NUEVO: hip√≥tesis de investigaci√≥n
            'hipotesis', 'hip√≥tesis', 'lineas de investigacion', 'l√≠neas de investigaci√≥n',
            'explicaciones plausibles', 'posibles explicaciones'
        ]
        
        if any(concepto in pregunta_lower for concepto in conceptos_complejos):
            logger.info("Pregunta conceptual compleja detectada - dirigiendo a RAG")
            return TipoConsulta.RAG
            
        try:
            with self.get_db_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("SELECT clasificar_tipo_consulta(%s)", (pregunta,))
                    tipo = cur.fetchone()[0]
                    return TipoConsulta(tipo)
        except Exception as e:
            logger.warning(f"Error clasificando consulta: {str(e)}")
            return TipoConsulta.HIBRIDA

    async def _resolver_consulta_frecuente(self, pregunta: str) -> RespuestaRAG:
        """Resolver consulta usando vistas materializadas"""
        logger.info("Resolviendo consulta frecuente con vistas materializadas")
        
        # Detectar qu√© vista materializada usar
        pregunta_lower = pregunta.lower()
        # Normalizar acentos para mejor detecci√≥n
        import unicodedata
        pregunta_norm = unicodedata.normalize('NFD', pregunta_lower)
        pregunta_norm = ''.join(c for c in pregunta_norm if unicodedata.category(c) != 'Mn')
        
        if any(palabra in pregunta_norm for palabra in ['dashboard', 'estadisticas', 'metricas', 'resumen']):
            return await self._generar_dashboard()
        elif any(palabra in pregunta_norm for palabra in ['departamento', 'geografia', 'lugar', 'territorial']):
            return await self._generar_analisis_geografico(pregunta)
        elif any(palabra in pregunta_norm for palabra in ['top', 'principales', 'mayores', 'mas mencionado']):
            return await self._generar_top_entidades(pregunta)
        elif any(palabra in pregunta_norm for palabra in ['cuantas', 'cuantos', 'cantidad', 'numero', 'total']):
            return await self._generar_conteo_entidades(pregunta)
        else:
            # Fallback a b√∫squeda RAG directa (evitar recursi√≥n con _resolver_consulta_hibrida)
            return await self._resolver_consulta_rag(pregunta)

    async def _generar_dashboard(self) -> RespuestaRAG:
        """Generar respuesta del dashboard principal"""
        try:
            with self.get_db_connection() as conn:
                with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                    cur.execute("SELECT get_dashboard_metricas() as metricas")
                    resultado = cur.fetchone()
                    
                    metricas = resultado['metricas']
                    
                    # Formatear respuesta
                    texto = f"""üìä **Dashboard Ejecutivo - Sistema de Documentos Judiciales**

üéØ **Estado del Procesamiento:**
‚Ä¢ Documentos procesados: {metricas['total_documentos']:,} ({metricas['progreso_procesamiento']}%)
‚Ä¢ Progreso: {metricas['progreso_procesamiento']:.1f}% de 11,446 documentos totales

üë• **Entidades Identificadas:**
‚Ä¢ Personas: {metricas['total_personas']:,} individuos √∫nicos
‚Ä¢ Organizaciones: {metricas['total_organizaciones']:,} entidades organizacionales  
‚Ä¢ Lugares: {metricas['total_lugares']:,} ubicaciones georreferenciadas
‚Ä¢ Casos √∫nicos: {metricas['casos_unicos']:,} NUC procesados

üèõÔ∏è **Clasificaci√≥n por Tipo:**
‚Ä¢ V√≠ctimas: {metricas['entidades_por_tipo']['victimas']:,}
‚Ä¢ Victimarios: {metricas['entidades_por_tipo']['victimarios']:,}
‚Ä¢ Defensa: {metricas['entidades_por_tipo']['defensa']:,}
‚Ä¢ Fiscales: {metricas['entidades_por_tipo']['fiscales']:,}
‚Ä¢ Fuerzas leg√≠timas: {metricas['entidades_por_tipo']['fuerzas_legitimas']:,}
‚Ä¢ Fuerzas ilegales: {metricas['entidades_por_tipo']['fuerzas_ilegales']:,}

‚è∞ **√öltima actualizaci√≥n:** {metricas['ultima_actualizacion']}

*Datos generados autom√°ticamente desde vistas materializadas optimizadas*"""

                    return RespuestaRAG(
                        texto=texto,
                        fuentes=[{"tipo": "vista_materializada", "nombre": "mv_dashboard_principal"}],
                        confianza=0.95,
                        metodo=MetodoResolucion.VISTA_MATERIALIZADA,
                        tiempo_respuesta=0,
                        datos_estructurados=metricas
                    )
        except Exception as e:
            logger.error(f"Error generando dashboard: {str(e)}")
            raise

    async def _generar_analisis_geografico(self, pregunta: str) -> RespuestaRAG:
        """Generar an√°lisis geogr√°fico"""
        try:
            # Extraer departamento si se menciona
            departamento = None
            for palabra in pregunta.split():
                if len(palabra) > 4 and palabra.lower() not in ['departamento', 'geografia']:
                    departamento = palabra
                    break
            
            with self.get_db_connection() as conn:
                with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                    if departamento:
                        cur.execute("SELECT * FROM get_analisis_geografico(%s) LIMIT 5", (departamento,))
                    else:
                        cur.execute("SELECT * FROM get_analisis_geografico() LIMIT 10")
                    
                    resultados = cur.fetchall()
                    
                    if not resultados:
                        texto = f"No se encontr√≥ informaci√≥n geogr√°fica para '{departamento}'" if departamento else "No hay datos geogr√°ficos disponibles"
                        return RespuestaRAG(texto=texto, fuentes=[], confianza=0.3, metodo=MetodoResolucion.VISTA_MATERIALIZADA, tiempo_respuesta=0)
                    
                    # Formatear respuesta
                    texto = "üó∫Ô∏è **An√°lisis Geogr√°fico**\n\n"
                    
                    for i, resultado in enumerate(resultados, 1):
                        texto += f"**{i}. {resultado['departamento']}**\n"
                        texto += f"‚Ä¢ Lugares espec√≠ficos: {resultado['lugares_especificos']:,}\n"
                        texto += f"‚Ä¢ Municipios afectados: {resultado['municipios_afectados']:,}\n"
                        texto += f"‚Ä¢ Total menciones: {resultado['total_menciones']:,}\n"
                        texto += f"‚Ä¢ Casos involucrados: {resultado['casos_involucrados']:,}\n"
                        
                        if resultado['top_lugares']:
                            top_lugares = resultado['top_lugares'][:3]
                            lugares_str = ", ".join([f"{lugar['lugar']} ({lugar['menciones']})" for lugar in top_lugares])
                            texto += f"‚Ä¢ Principales lugares: {lugares_str}\n"
                        
                        texto += "\n"
                    
                    texto += "*An√°lisis basado en vistas materializadas optimizadas*"
                    
                    return RespuestaRAG(
                        texto=texto,
                        fuentes=[{"tipo": "vista_materializada", "nombre": "mv_analisis_geografico"}],
                        confianza=0.9,
                        metodo=MetodoResolucion.VISTA_MATERIALIZADA,
                        tiempo_respuesta=0,
                        datos_estructurados={"resultados": [dict(r) for r in resultados]}
                    )
        except Exception as e:
            logger.error(f"Error en an√°lisis geogr√°fico: {str(e)}")
            raise

    async def _generar_top_entidades(self, pregunta: str) -> RespuestaRAG:
        """Generar top de entidades"""
        try:
            # Detectar qu√© tipo de entidad busca
            pregunta_lower = pregunta.lower()
            
            if 'persona' in pregunta_lower or 'gente' in pregunta_lower:
                tipo_filtro = 'persona'
            elif 'organizacion' in pregunta_lower or 'grupo' in pregunta_lower:
                tipo_filtro = 'organizacion'
            elif 'lugar' in pregunta_lower or 'sitio' in pregunta_lower:
                tipo_filtro = 'lugar'
            else:
                tipo_filtro = None
            
            with self.get_db_connection() as conn:
                with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                    if tipo_filtro:
                        cur.execute("""
                            SELECT * FROM mv_top_entidades 
                            WHERE tipo_entidad = %s 
                            ORDER BY frecuencia DESC 
                            LIMIT 15
                        """, (tipo_filtro,))
                    else:
                        cur.execute("""
                            SELECT * FROM mv_top_entidades 
                            ORDER BY frecuencia DESC 
                            LIMIT 20
                        """)
                    
                    resultados = cur.fetchall()
                    
                    if not resultados:
                        return RespuestaRAG(
                            texto="No se encontraron entidades frecuentes para mostrar",
                            fuentes=[], confianza=0.3,
                            metodo=MetodoResolucion.VISTA_MATERIALIZADA, tiempo_respuesta=0
                        )
                    
                    # Agrupar por tipo
                    por_tipo = {}
                    for resultado in resultados:
                        tipo = resultado['tipo_entidad']
                        if tipo not in por_tipo:
                            por_tipo[tipo] = []
                        por_tipo[tipo].append(resultado)
                    
                    # Formatear respuesta
                    texto = "üèÜ **Entidades M√°s Mencionadas**\n\n"
                    
                    iconos = {
                        'persona': 'üë§',
                        'organizacion': 'üèõÔ∏è',
                        'lugar': 'üìç'
                    }
                    
                    for tipo, entidades in por_tipo.items():
                        icono = iconos.get(tipo, 'üìã')
                        texto += f"{icono} **{tipo.title()}s:**\n"
                        
                        for i, entidad in enumerate(entidades[:10], 1):
                            texto += f"{i}. {entidad['entidad']} ({entidad['frecuencia']} menciones)\n"
                        
                        texto += "\n"
                    
                    texto += "*Basado en an√°lisis de frecuencia de menciones*"
                    
                    return RespuestaRAG(
                        texto=texto,
                        fuentes=[{"tipo": "vista_materializada", "nombre": "mv_top_entidades"}],
                        confianza=0.9,
                        metodo=MetodoResolucion.VISTA_MATERIALIZADA,
                        tiempo_respuesta=0,
                        datos_estructurados={"entidades_por_tipo": por_tipo}
                    )
        except Exception as e:
            logger.error(f"Error generando top entidades: {str(e)}")
            raise

    async def _generar_conteo_entidades(self, pregunta: str) -> RespuestaRAG:
        """Generar conteo espec√≠fico de entidades"""
        try:
            # Normalizar pregunta quitando acentos
            import unicodedata
            pregunta_lower = pregunta.lower()
            pregunta_norm = unicodedata.normalize('NFD', pregunta_lower)
            pregunta_norm = ''.join(c for c in pregunta_norm if unicodedata.category(c) != 'Mn')
            
            with self.get_db_connection() as conn:
                with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                    
                    # Detectar qu√© tipo de conteo se requiere
                    if 'victima' in pregunta_norm:
                        cur.execute("""
                            SELECT COUNT(DISTINCT nombre) as total 
                            FROM personas 
                            WHERE tipo ILIKE '%victima%' AND tipo NOT ILIKE '%victimario%'
                        """)
                        resultado = cur.fetchone()
                        entidad = "v√≠ctimas √∫nicas"
                        total = resultado['total']
                        
                        # Desglose adicional
                        cur.execute("""
                            SELECT tipo, COUNT(DISTINCT nombre) as cantidad
                            FROM personas 
                            WHERE tipo ILIKE '%victima%' AND tipo NOT ILIKE '%victimario%'
                            GROUP BY tipo
                            ORDER BY cantidad DESC
                        """)
                        desglose = cur.fetchall()
                        
                    elif 'victimario' in pregunta_norm:
                        cur.execute("""
                            SELECT COUNT(DISTINCT nombre) as total 
                            FROM personas 
                            WHERE tipo ILIKE '%victimario%'
                        """)
                        resultado = cur.fetchone()
                        entidad = "victimarios √∫nicos"
                        total = resultado['total']
                        
                        cur.execute("""
                            SELECT tipo, COUNT(DISTINCT nombre) as cantidad
                            FROM personas 
                            WHERE tipo ILIKE '%victimario%'
                            GROUP BY tipo
                            ORDER BY cantidad DESC
                        """)
                        desglose = cur.fetchall()
                        
                    elif 'persona' in pregunta_norm:
                        cur.execute("SELECT COUNT(DISTINCT nombre) as total FROM personas")
                        resultado = cur.fetchone()
                        entidad = "personas √∫nicas"
                        total = resultado['total']
                        
                        cur.execute("""
                            SELECT tipo, COUNT(DISTINCT nombre) as cantidad
                            FROM personas 
                            GROUP BY tipo
                            ORDER BY cantidad DESC
                            LIMIT 10
                        """)
                        desglose = cur.fetchall()
                        
                    elif 'organizacion' in pregunta_norm:
                        cur.execute("SELECT COUNT(DISTINCT nombre) as total FROM organizaciones")
                        resultado = cur.fetchone()
                        entidad = "organizaciones √∫nicas"
                        total = resultado['total']
                        
                        cur.execute("""
                            SELECT tipo, COUNT(DISTINCT nombre) as cantidad
                            FROM organizaciones 
                            GROUP BY tipo
                            ORDER BY cantidad DESC
                            LIMIT 10
                        """)
                        desglose = cur.fetchall()
                        
                    elif 'documento' in pregunta_norm:
                        cur.execute("SELECT COUNT(*) as total FROM documentos")
                        resultado = cur.fetchone()
                        entidad = "documentos"
                        total = resultado['total']
                        
                        cur.execute("""
                            SELECT 
                                DATE_PART('year', fecha_proceso) as a√±o,
                                COUNT(*) as cantidad
                            FROM documentos 
                            WHERE fecha_proceso IS NOT NULL
                            GROUP BY DATE_PART('year', fecha_proceso)
                            ORDER BY a√±o DESC
                            LIMIT 5
                        """)
                        desglose = cur.fetchall()
                        
                    else:
                        # Conteo general
                        cur.execute("""
                            SELECT 
                                'documentos' as tipo, (SELECT COUNT(*) FROM documentos) as total
                            UNION ALL
                            SELECT 
                                'personas √∫nicas' as tipo, (SELECT COUNT(DISTINCT nombre) FROM personas) as total
                            UNION ALL
                            SELECT 
                                'organizaciones √∫nicas' as tipo, (SELECT COUNT(DISTINCT nombre) FROM organizaciones) as total
                        """)
                        totales = cur.fetchall()
                        
                        texto = "üìä **Resumen de Entidades en el Sistema**\n\n"
                        for item in totales:
                            texto += f"‚Ä¢ {item['tipo'].title()}: **{item['total']:,}**\n"
                        
                        texto += f"\n*Datos actualizados autom√°ticamente desde la base de datos*"
                        
                        return RespuestaRAG(
                            texto=texto,
                            fuentes=[{"tipo": "consulta_directa", "tabla": "conteo_general"}],
                            confianza=0.99,
                            metodo=MetodoResolucion.VISTA_MATERIALIZADA,
                            tiempo_respuesta=0,
                            datos_estructurados={"totales": [dict(t) for t in totales]}
                        )
                    
                    # Formatear respuesta espec√≠fica
                    texto = f"üìä **Total de {entidad.title()}**: **{total:,}**\n\n"
                    
                    if desglose:
                        texto += f"üìã **Desglose por tipo:**\n"
                        for item in desglose[:8]:  # Top 8
                            tipo_nombre = item.get('tipo', item.get('a√±o', 'N/A'))
                            cantidad = item.get('cantidad', 0)
                            texto += f"‚Ä¢ {tipo_nombre}: {cantidad:,}\n"
                        
                        if len(desglose) > 8:
                            texto += f"‚Ä¢ ... y {len(desglose) - 8} tipos m√°s\n"
                    
                    texto += f"\n*Consulta optimizada ejecutada directamente en la base de datos*"
                    
                    return RespuestaRAG(
                        texto=texto,
                        fuentes=[{"tipo": "consulta_directa", "tabla": "conteo_" + entidad}],
                        confianza=0.99,
                        metodo=MetodoResolucion.VISTA_MATERIALIZADA,
                        tiempo_respuesta=0,
                        datos_estructurados={
                            "total": convert_db_types(total),
                            "entidad": entidad,
                            "desglose": [convert_db_types(dict(d)) for d in desglose]
                        }
                    )
                    
        except Exception as e:
            logger.error(f"Error generando conteo de entidades: {str(e)}")
            raise

    async def _resolver_consulta_rag(self, pregunta: str) -> RespuestaRAG:
        """Resolver consulta compleja usando RAG con Azure Search + LLM"""
        logger.info("Resolviendo consulta compleja con RAG + LLM")
        
        try:
            # 1. Buscar primero en Azure Search (vectorizado/sem√°ntico)
            contexto_azure = []
            try:
                azure_search = AzureSearchVectorizado()
                chunks_azure = await azure_search.buscar_semanticamente(pregunta, top_k=5)
                
                if chunks_azure:
                    logger.info(f"Azure Search encontr√≥ {len(chunks_azure)} chunks relevantes")
                    for chunk in chunks_azure:
                        # Extraer metadatos de ubicaci√≥n
                        pagina = chunk.metadata.get('pagina', 'N/A') if hasattr(chunk, 'metadata') else 'N/A'
                        parrafo = chunk.metadata.get('parrafo', 'N/A') if hasattr(chunk, 'metadata') else 'N/A'
                        
                        contexto_azure.append({
                            'texto': chunk.contenido if hasattr(chunk, 'contenido') else str(chunk),
                            'fuente': f"Archivo: {chunk.nombre_archivo if hasattr(chunk, 'nombre_archivo') else 'N/A'} - {chunk.tipo_documental if hasattr(chunk, 'tipo_documental') else 'Documento'}",
                            'relevancia': chunk.score if hasattr(chunk, 'score') else 0.0,
                            'tipo': 'azure_search',
                            'analisis': chunk.analisis if hasattr(chunk, 'analisis') else '',
                            'pagina': pagina,
                            'parrafo': parrafo,
                            'nombre_archivo': chunk.nombre_archivo if hasattr(chunk, 'nombre_archivo') else 'N/A',
                            'expediente_nuc': chunk.expediente_nuc if hasattr(chunk, 'expediente_nuc') else 'N/A',
                            'tipo_documental': chunk.tipo_documental if hasattr(chunk, 'tipo_documental') else 'N/A'
                        })
                else:
                    logger.warning("Azure Search no encontr√≥ chunks relevantes")
            except Exception as e:
                logger.warning(f"Error con Azure Search: {str(e)}")
            
            # 2. Si Azure Search no encuentra suficiente, buscar en PostgreSQL
            contexto_sql = []
            if len(contexto_azure) < 3:
                logger.info("Complementando con b√∫squeda SQL")
                terminos_clave = await self._extraer_terminos_clave(pregunta)
                resultado_sql = await self._buscar_contexto_sql(terminos_clave, pregunta)
                
                # Convertir resultado SQL a formato de lista
                if resultado_sql and isinstance(resultado_sql, dict):
                    # Extraer informaci√≥n relevante del resultado SQL
                    for key, value in resultado_sql.items():
                        if isinstance(value, list) and value:
                            for item in value[:3]:  # M√°ximo 3 items por categor√≠a
                                contexto_sql.append({
                                    'texto': str(item),
                                    'fuente': f"PostgreSQL - {key}",
                                    'relevancia': 0.5,
                                    'tipo': 'sql_search'
                                })
            
            # 3. Combinar contextos
            contexto_completo = contexto_azure + contexto_sql
            
            # 4. Generar respuesta con LLM
            respuesta_llm = await self._generar_respuesta_llm(pregunta, contexto_completo)
            
            return respuesta_llm
            
        except Exception as e:
            logger.error(f"Error en consulta RAG: {str(e)}")
            raise

    async def _extraer_terminos_clave(self, pregunta: str) -> List[str]:
        """Extraer t√©rminos clave de la pregunta usando t√©cnicas simples"""
        # Implementaci√≥n simple - en producci√≥n se podr√≠a usar NLP m√°s avanzado
        palabras_clave = []
        
        # Buscar nombres propios (palabras que empiezan con may√∫scula)
        import re
        nombres_propios = re.findall(r'\b[A-Z][a-z√°√©√≠√≥√∫√±]+\b', pregunta)
        palabras_clave.extend(nombres_propios)
        
        # Buscar siglas
        siglas = re.findall(r'\b[A-Z]{2,}\b', pregunta)
        palabras_clave.extend(siglas)
        
        # Palabras importantes (sustantivos comunes en el dominio)
        palabras_importantes = [
            'victima', 'victimas', 'victimario', 'victimarios', 'defensa',
            'fiscal', 'juez', 'organizacion', 'grupo', 'ejercito', 'policia',
            'lugar', 'municipio', 'departamento', 'caso', 'documento'
        ]
        
        for palabra in palabras_importantes:
            if palabra in pregunta.lower():
                palabras_clave.append(palabra)
        
        return list(set(palabras_clave))  # Eliminar duplicados

    async def _buscar_contexto_sql(self, terminos_clave: List[str], pregunta_original: str) -> Dict[str, Any]:
        """Buscar contexto relevante usando las funciones RAG de SQL"""
        contexto = {
            'personas': [],
            'organizaciones': [],
            'lugares': [],
            'total_fuentes': 0
        }
        
        try:
            with self.get_db_connection() as conn:
                with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                    
                    # Buscar personas relevantes
                    if terminos_clave:
                        cur.execute("""
                            SELECT * FROM rag_buscar_contexto_personas(%s::text[], 10)
                        """, (terminos_clave,))
                        contexto['personas'] = [dict(row) for row in cur.fetchall()]
                    
                    # Buscar organizaciones relevantes
                    if terminos_clave:
                        cur.execute("""
                            SELECT * FROM rag_buscar_contexto_organizaciones(%s::text[], 10)
                        """, (terminos_clave,))
                        contexto['organizaciones'] = [dict(row) for row in cur.fetchall()]
                    
                    # Buscar lugares relevantes
                    if terminos_clave:
                        cur.execute("""
                            SELECT * FROM rag_buscar_contexto_geografico(%s::text[], 10)
                        """, (terminos_clave,))
                        contexto['lugares'] = [dict(row) for row in cur.fetchall()]
            
            contexto['total_fuentes'] = (
                len(contexto['personas']) + 
                len(contexto['organizaciones']) + 
                len(contexto['lugares'])
            )
            
            logger.info(f"Contexto encontrado: {contexto['total_fuentes']} fuentes")
            return contexto
            
        except Exception as e:
            logger.warning(f"Error buscando contexto SQL: {str(e)}")
            return contexto

    async def _generar_respuesta_llm(self, pregunta: str, contexto) -> RespuestaRAG:
        """Generar respuesta usando Azure OpenAI"""
        try:
            # Manejar tanto listas como diccionarios
            if isinstance(contexto, list):
                contexto_str = self._formatear_contexto_lista_para_llm(contexto)
            else:
                contexto_str = self._formatear_contexto_para_llm(contexto)
            
            pregunta_lower = pregunta.lower()
            if any(k in pregunta_lower for k in ['hip√≥tesis', 'hipotesis', 'l√≠neas de investigaci√≥n', 'lineas de investigacion', 'posibles explicaciones', 'explicaciones plausibles']):
                # Modo generaci√≥n de hip√≥tesis de investigaci√≥n
                system_prompt = """Eres un analista legal senior de la Fiscal√≠a.
Tu tarea es CONSTRUIR HIP√ìTESIS DE INVESTIGACI√ìN bas√°ndote √∫nicamente en el contexto proporcionado, con m√°xima trazabilidad.

REGLAS:
1. Prop√≥n 3 a 5 hip√≥tesis claras, contrastables y accionables.
2. Cada hip√≥tesis debe incluir: descripci√≥n, evidencias de soporte [CITA-X], supuestos, se√±ales en contra, c√≥mo refutar/confirmar, pr√≥ximos pasos.
3. Usa lenguaje probabil√≠stico (posiblemente, es plausible, podr√≠a indicar), evita afirmaciones categ√≥ricas.
4. Mant√©n rigor legal y referencia toda evidencia con [CITA-X]."""

                user_prompt = f"""PREGUNTA: {pregunta}

CONTEXTO DISPONIBLE:
{contexto_str}

Devuelve en Markdown:

## Hip√≥tesis de Investigaci√≥n

1) [T√≠tulo breve]
   - Descripci√≥n: ...
   - Evidencias de soporte: ... [CITA-1], [CITA-2]
   - Supuestos: ...
   - Se√±ales en contra: ...
   - C√≥mo refutar/confirmar: ...
   - Pr√≥ximos pasos (acciones): ...

2) ... (3 a 5 hip√≥tesis en total)

Al final incluye:
REFERENCIAS:
[CITA-X] Fuente y localizaci√≥n (archivo/p√°gina/p√°rrafo)"""
            else:
                # Modo respuesta est√°ndar con citas
                system_prompt = """Eres un asistente legal especializado en an√°lisis de documentos judiciales del caso UP (Uni√≥n Patri√≥tica).
Tu tarea es responder preguntas bas√°ndote √∫nicamente en el contexto proporcionado con M√ÅXIMA TRAZABILIDAD.

INSTRUCCIONES OBLIGATORIAS PARA CITAS LEGALES:
1. CADA afirmaci√≥n DEBE estar respaldada con su cita exacta usando el formato [CITA-X]
2. Responde SOLO con informaci√≥n del contexto proporcionado
3. Incluye citas textuales entrecomilladas cuando sea relevante
4. Si no hay informaci√≥n suficiente para una afirmaci√≥n, dilo claramente
5. Mant√©n el rigor legal y la precisi√≥n documental
6. Estructura la respuesta de manera profesional y clara

FORMATO DE CITAS REQUERIDO:
- Despu√©s de cada afirmaci√≥n: [CITA-X] donde X es el n√∫mero de referencia
- Para citas textuales: "texto exacto" [CITA-X]
- Ejemplo: La Uni√≥n Patri√≥tica fue objeto de persecuci√≥n sistem√°tica [CITA-1].

RECUERDA: Este es un sistema legal que requiere trazabilidad m√°xima. Cada dato debe ser verificable."""

                user_prompt = f"""PREGUNTA: {pregunta}

CONTEXTO DISPONIBLE:
{contexto_str}

Responde a la pregunta bas√°ndote √∫nicamente en el contexto proporcionado.

ESTRUCTURA REQUERIDA DE LA RESPUESTA:
1. Respuesta principal con citas [CITA-X] despu√©s de cada afirmaci√≥n
2. Al final, incluye una secci√≥n "REFERENCIAS:" listando todas las citas usadas

Ejemplo de formato:
La Uni√≥n Patri√≥tica fue perseguida sistem√°ticamente [CITA-1]. Esto constituye genocidio seg√∫n la jurisprudencia [CITA-2].

REFERENCIAS:
[CITA-1] Archivo: sentencia_123.pdf, P√°gina XX, P√°rrafo XX
[CITA-2] Archivo: auto_456.pdf, P√°gina YY, P√°rrafo YY"""

            # Llamada a Azure OpenAI
            start_time = time.time()
            
            response = self.azure_client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=1500
            )
            
            tiempo_llm = int((time.time() - start_time) * 1000)
            
            respuesta_texto = response.choices[0].message.content
            
            # Calcular m√©tricas
            tokens_prompt = response.usage.prompt_tokens
            tokens_respuesta = response.usage.completion_tokens
            costo_estimado = (tokens_prompt * 0.000150 + tokens_respuesta * 0.000600) / 1000  # Precios GPT-4o-mini
            
            # Calcular confianza basada en cantidad de contexto
            if isinstance(contexto, list):
                total_fuentes = len(contexto)
                azure_chunks = [item for item in contexto if item.get('tipo') == 'azure_search']
                if azure_chunks:
                    confianza = min(0.9, 0.6 + (len(azure_chunks) * 0.05))  # Base m√°s alta para Azure Search
                else:
                    confianza = min(0.9, 0.5 + (total_fuentes * 0.05))
            elif 'chunks_azure' in contexto:
                total_fuentes = len(contexto['chunks_azure'])
                confianza = min(0.9, 0.6 + (total_fuentes * 0.05))  # Base m√°s alta para Azure Search
            else:
                total_fuentes = contexto.get('total_fuentes', 0)
                confianza = min(0.9, 0.5 + (total_fuentes * 0.05))
            
            # Preparar fuentes
            fuentes = []
            
            # Manejar fuentes de lista (nuevo formato)
            if isinstance(contexto, list):
                for i, item in enumerate(contexto[:5], 1):  # M√°ximo 5 fuentes
                    if isinstance(item, dict):
                        fuente_info = {
                            'cita': f'CITA-{i}',
                            'nombre_archivo': item.get('nombre_archivo', 'N/A'),
                            'expediente': item.get('expediente_nuc', 'N/A'),
                            'tipo_documento': item.get('tipo_documental', 'N/A'),
                            'pagina': item.get('pagina', 'N/A'),
                            'parrafo': item.get('parrafo', 'N/A'),
                            'relevancia': item.get('relevancia', 0.0),
                            'texto_fuente': item.get('texto', ''),  # Guardar texto completo para mostrar en UI
                            'texto_resumen': item.get('texto', '')[:200] + '...' if len(item.get('texto', '')) > 200 else item.get('texto', '')
                        }
                        fuentes.append(fuente_info)
            # Manejar fuentes de Azure Search (formato anterior)
            elif 'chunks_azure' in contexto:
                for chunk in contexto['chunks_azure'][:3]:
                    fuentes.append({
                        'tipo': 'documento_vectorizado',
                        'expediente': getattr(chunk, 'expediente_nuc', 'N/A'),
                        'tipo_documental': getattr(chunk, 'tipo_documental', 'N/A'),
                        'relevancia': getattr(chunk, 'score', 0)
                    })
            else:
                # Manejar fuentes tradicionales SQL (contexto como diccionario)
                if isinstance(contexto, dict):
                    for categoria, items in contexto.items():
                        if categoria != 'total_fuentes' and items:
                            for item in items[:3]:  # Primeras 3 de cada categor√≠a
                                if isinstance(item, dict):
                                    fuentes.append({
                                        'tipo': categoria,
                                        'entidad': item.get('persona') or item.get('organizacion') or item.get('lugar', 'N/A'),
                                        'relevancia': item.get('score_relevancia', 0)
                                    })
            
            metadatos = {
                'model': self.deployment_name,
                'tokens_prompt': tokens_prompt,
                'tokens_respuesta': tokens_respuesta,
                'costo_estimado': costo_estimado,
                'tiempo_llm_ms': tiempo_llm,
                'temperatura': 0.3
            }
            
            return RespuestaRAG(
                texto=respuesta_texto,
                fuentes=fuentes,
                confianza=confianza,
                metodo=MetodoResolucion.LLM_GENERACION,
                tiempo_respuesta=tiempo_llm,
                metadatos_llm=metadatos
            )
            
        except Exception as e:
            logger.error(f"Error generando respuesta LLM: {str(e)}")
            raise

    def _formatear_contexto_para_llm(self, contexto: Dict[str, Any]) -> str:
        """Formatear contexto para enviar al LLM"""
        partes = []
        
        # Manejar chunks de Azure Search
        if 'chunks_azure' in contexto:
            partes.append("DOCUMENTOS RELEVANTES ENCONTRADOS:")
            for i, chunk in enumerate(contexto['chunks_azure'][:5], 1):
                expediente = getattr(chunk, 'expediente_nuc', 'N/A')
                tipo_doc = getattr(chunk, 'tipo_documental', 'N/A')
                contenido = getattr(chunk, 'contenido', '')
                analisis = getattr(chunk, 'analisis', '')
                
                partes.append(f"\n{i}. EXPEDIENTE: {expediente}")
                partes.append(f"   TIPO: {tipo_doc}")
                if contenido:
                    partes.append(f"   CONTENIDO: {contenido[:300]}...")
                if analisis:
                    partes.append(f"   AN√ÅLISIS: {analisis[:300]}...")
            partes.append("")
        
        # Manejar contexto tradicional SQL
        if contexto.get('personas'):
            partes.append("PERSONAS RELEVANTES:")
            for persona in contexto['personas'][:5]:
                partes.append(f"- {persona['persona']} ({persona['tipo']}): {persona['contexto'][:200]}...")
            partes.append("")
        
        if contexto.get('organizaciones'):
            partes.append("ORGANIZACIONES RELEVANTES:")
            for org in contexto['organizaciones'][:5]:
                partes.append(f"- {org['organizacion']} ({org['tipo']}): {org['contexto'][:200]}...")
            partes.append("")
        
        if contexto.get('lugares'):
            partes.append("LUGARES RELEVANTES:")
            for lugar in contexto['lugares'][:5]:
                partes.append(f"- {lugar['lugar']} ({lugar['departamento']}): {lugar['contexto'][:200]}...")
            partes.append("")
        
        return "\n".join(partes)

    def _formatear_contexto_lista_para_llm(self, contexto_lista) -> str:
        """Formatear lista de contexto para enviar al LLM con citas detalladas"""
        partes = []
        
        if not contexto_lista:
            return "No se encontr√≥ informaci√≥n relevante en los documentos."
        
        partes.append("INFORMACI√ìN ENCONTRADA EN LOS DOCUMENTOS JUDICIALES:")
        partes.append("INSTRUCCIONES PARA CITAS: Cada afirmaci√≥n DEBE incluir la cita exacta con formato [CITA-X] donde X es el n√∫mero de referencia.")
        partes.append("")
        
        for i, item in enumerate(contexto_lista[:10], 1):  # M√°ximo 10 items
            if isinstance(item, dict):
                # Extraer informaci√≥n de ubicaci√≥n
                pagina = item.get('pagina', 'N/A')
                parrafo = item.get('parrafo', 'N/A')
                nombre_archivo = item.get('nombre_archivo', 'N/A')
                tipo_doc = item.get('tipo_documental', 'N/A')
                expediente = item.get('expediente_nuc', 'N/A')
                
                partes.append(f"[CITA-{i}]")
                partes.append(f"ARCHIVO: {nombre_archivo}")
                partes.append(f"TIPO DOCUMENTO: {tipo_doc}")
                partes.append(f"EXPEDIENTE NUC: {expediente}")
                partes.append(f"UBICACI√ìN: P√°gina {pagina}, P√°rrafo {parrafo}")
                partes.append(f"RELEVANCIA: {item.get('relevancia', 0.0):.2f}")
                partes.append(f"TEXTO EXACTO:")
                
                texto = item.get('texto', '')
                if texto:
                    # Mantener el texto completo para citas exactas
                    partes.append(f'"{texto}"')
                
                analisis = item.get('analisis', '')
                if analisis:
                    partes.append(f"RESUMEN DEL AN√ÅLISIS: {analisis}")
                
                partes.append("")
            else:
                partes.append(f"[CITA-{i}] {str(item)[:500]}...")
                partes.append("")
        
        partes.append("IMPORTANTE: En tu respuesta, SIEMPRE incluye las citas usando el formato [CITA-X] despu√©s de cada afirmaci√≥n.")
        partes.append("Ejemplo: 'La Uni√≥n Patri√≥tica fue perseguida sistem√°ticamente [CITA-1] y esto constituye genocidio seg√∫n la jurisprudencia [CITA-2].'")
        
        return "\n".join(partes)

    async def _resolver_consulta_hibrida(self, pregunta: str) -> RespuestaRAG:
        """Resolver consulta h√≠brida combinando vistas materializadas y RAG"""
        logger.info("Resolviendo consulta h√≠brida")
        
        # Detectar si la pregunta es del tipo que se puede resolver con vistas materializadas
        pregunta_lower = pregunta.lower()
        import unicodedata
        pregunta_norm = unicodedata.normalize('NFD', pregunta_lower)
        pregunta_norm = ''.join(c for c in pregunta_norm if unicodedata.category(c) != 'Mn')
        
        # Si es una consulta espec√≠fica para vistas materializadas, probar primero
        if any(palabra in pregunta_norm for palabra in ['dashboard', 'estadisticas', 'metricas', 'resumen', 'departamento', 'geografia', 'lugar', 'territorial', 'top', 'principales', 'mayores', 'mas mencionado', 'cuantas', 'cuantos', 'cantidad', 'numero', 'total']):
            try:
                logger.info("Intentando resoluci√≥n con vistas materializadas en h√≠brida")
                if any(palabra in pregunta_norm for palabra in ['dashboard', 'estadisticas', 'metricas', 'resumen']):
                    respuesta_vm = await self._generar_dashboard()
                elif any(palabra in pregunta_norm for palabra in ['departamento', 'geografia', 'lugar', 'territorial']):
                    respuesta_vm = await self._generar_analisis_geografico(pregunta)
                elif any(palabra in pregunta_norm for palabra in ['top', 'principales', 'mayores', 'mas mencionado']):
                    respuesta_vm = await self._generar_top_entidades(pregunta)
                elif any(palabra in pregunta_norm for palabra in ['cuantas', 'cuantos', 'cantidad', 'numero', 'total']):
                    respuesta_vm = await self._generar_conteo_entidades(pregunta)
                else:
                    respuesta_vm = None
                
                if respuesta_vm and respuesta_vm.confianza >= 0.7:
                    return respuesta_vm
            except Exception as e:
                logger.warning(f"Error en vistas materializadas h√≠brida: {str(e)}")
        
        # Si no funciona o no es del tipo apropiado, usar RAG
        return await self._resolver_consulta_rag(pregunta)

    async def _guardar_cache(self, pregunta: str, respuesta: RespuestaRAG):
        """Guardar respuesta en cache"""
        try:
            with self.get_db_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        SELECT guardar_respuesta_cache(%s, %s, %s)
                    """, (pregunta, respuesta.texto, json.dumps(convert_db_types(respuesta.fuentes))))
                    logger.info("Respuesta guardada en cache")
        except Exception as e:
            logger.warning(f"Error guardando en cache: {str(e)}")

    async def registrar_feedback(self, consulta_id: int, respuesta_id: int, feedback: FeedbackRAG):
        """Registrar feedback del usuario"""
        try:
            with self.get_db_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        SELECT registrar_feedback_rag(%s, %s, %s, %s, %s, %s, %s)
                    """, (
                        consulta_id, respuesta_id, feedback.calificacion,
                        feedback.comentario, json.dumps(convert_db_types(feedback.aspectos)) if feedback.aspectos else None,
                        feedback.respuesta_esperada, None
                    ))
                    
                    feedback_id = cur.fetchone()[0]
                    logger.info(f"Feedback registrado con ID: {feedback_id}")
                    return feedback_id
        except Exception as e:
            logger.error(f"Error registrando feedback: {str(e)}")
            raise

    async def obtener_estadisticas_mejora_continua(self, dias: int = 30) -> Dict[str, Any]:
        """Obtener estad√≠sticas para mejora continua"""
        try:
            with self.get_db_connection() as conn:
                with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                    # Reporte de mejora continua
                    cur.execute("SELECT * FROM generar_reporte_mejora_continua(%s)", (dias,))
                    reporte = [dict(row) for row in cur.fetchall()]
                    
                    # Preguntas a optimizar
                    cur.execute("SELECT * FROM detectar_preguntas_optimizar()")
                    preguntas_optimizar = [dict(row) for row in cur.fetchall()]
                    
                    return {
                        'reporte_mejora': reporte,
                        'preguntas_optimizar': preguntas_optimizar,
                        'fecha_analisis': datetime.now().isoformat()
                    }
        except Exception as e:
            logger.error(f"Error obteniendo estad√≠sticas: {str(e)}")
            raise


# Funci√≥n principal para testing
async def main():
    """Funci√≥n de prueba del sistema RAG"""
    sistema = SistemaRAGTrazable()
    
    # Ejemplo de consulta
    consulta = ConsultaRAG(
        usuario_id="test_user",
        pregunta="¬øCu√°les son las estad√≠sticas principales del caso?",
        ip_cliente="127.0.0.1"
    )
    
    respuesta, consulta_id = await sistema.procesar_consulta(consulta)
    
    print("=== RESPUESTA ===")
    print(respuesta.texto)
    print(f"\nConfianza: {respuesta.confianza}")
    print(f"M√©todo: {respuesta.metodo.value}")
    print(f"Tiempo: {respuesta.tiempo_respuesta}ms")
    print(f"Fuentes: {len(respuesta.fuentes)}")
    
    # Ejemplo de feedback
    feedback = FeedbackRAG(
        calificacion=5,
        comentario="Excelente respuesta, muy completa",
        aspectos={"precision": 5, "relevancia": 5, "completitud": 4}
    )
    
    await sistema.registrar_feedback(consulta_id, 1, feedback)
    print("\nFeedback registrado")

# Funci√≥n p√∫blica s√≠ncrona para integraci√≥n f√°cil
def consulta_hibrida_sincrona(pregunta: str) -> Dict[str, Any]:
    """
    Funci√≥n s√≠ncrona para facilitar integraci√≥n con interfaces que no usan async
    Retorna respuesta RAG con trazabilidad completa de Azure Search
    """
    try:
        # Ejecutar la consulta usando asyncio
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        async def _ejecutar_consulta():
            try:
                # Cargar variables de entorno
                load_dotenv()
                load_dotenv('config/.env')
                
                # Usar directamente el Azure Search
                azure_search = AzureSearchVectorizado()
                chunks_azure = await azure_search.buscar_semanticamente(pregunta, top_k=5)
                
                # Formatear respuesta
                fuentes_formateadas = []
                if chunks_azure:
                    for i, chunk in enumerate(chunks_azure, 1):
                        # Los chunks vienen como objetos DocumentoChunk, no diccionarios
                        try:
                            # Extraer nombre de archivo m√°s legible
                            nombre_archivo = chunk.nombre_archivo if hasattr(chunk, 'nombre_archivo') else 'N/A'
                            if nombre_archivo.endswith('.json'):
                                # Extraer informaci√≥n m√°s descriptiva del nombre del archivo
                                partes = nombre_archivo.replace('.json', '').split('_')
                                if len(partes) >= 3:
                                    fecha = partes[0] if partes[0].isdigit() else 'S/F'
                                    tipo_doc = partes[1] if len(partes) > 1 else 'DOC'
                                    nombre_archivo = f"{tipo_doc}_{fecha}"
                            
                            # Limpiar y mejorar el contenido del chunk
                            contenido = chunk.contenido if hasattr(chunk, 'contenido') else ''
                            contenido_limpio = contenido.replace('\\n', ' ').replace('\\t', ' ')
                            
                            # Obtener metadatos
                            pagina = chunk.metadata.get('pagina', 'N/A') if hasattr(chunk, 'metadata') and chunk.metadata else 'N/A'
                            parrafo = chunk.metadata.get('parrafo', 'N/A') if hasattr(chunk, 'metadata') and chunk.metadata else 'N/A'
                            
                            fuentes_formateadas.append({
                                'archivo': nombre_archivo,
                                'contenido': contenido_limpio[:500] + '...' if len(contenido_limpio) > 500 else contenido_limpio,
                                'score': float(chunk.score if hasattr(chunk, 'score') else 0.0),
                                'pagina': pagina,
                                'parrafo': parrafo,
                                'tipo_documental': chunk.tipo_documental if hasattr(chunk, 'tipo_documental') else 'Documento Judicial',
                                'expediente_nuc': chunk.expediente_nuc if hasattr(chunk, 'expediente_nuc') else 'N/A',
                                'metadata': chunk.metadata if hasattr(chunk, 'metadata') else {},
                                'doc_ref': f"Doc {i}"  # Para las citas en el texto
                            })
                        except Exception as e:
                            logging.error(f"Error procesando chunk {i}: {e}")
                            # Fallback para chunks con estructura inesperada
                            fuentes_formateadas.append({
                                'archivo': f'Documento {i}',
                                'contenido': str(chunk)[:200] + '...',
                                'score': 0.0,
                                'pagina': 'N/A',
                                'parrafo': 'N/A',
                                'tipo_documental': 'Documento',
                                'expediente_nuc': 'N/A',
                                'metadata': {},
                                'doc_ref': f"Doc {i}"
                            })
                
                # Generar respuesta usando OpenAI si hay fuentes
                if fuentes_formateadas:
                    import httpx
                    from openai import AzureOpenAI
                    
                    http_client = httpx.Client(timeout=30.0)
                    azure_client = AzureOpenAI(
                        api_key=os.getenv('AZURE_OPENAI_API_KEY'),
                        api_version=os.getenv('AZURE_OPENAI_VERSION', '2024-12-01-preview'),
                        azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),
                        http_client=http_client
                    )
                    
                    # Crear contexto para la respuesta con referencias numeradas
                    contexto_chunks = '\n\n'.join([
                        f"[Doc {i+1}] **{f['tipo_documental']}** - {f['archivo']}\n"
                        f"**Ubicaci√≥n:** P√°gina {f['pagina']}, P√°rrafo {f['parrafo']} | **Expediente:** {f['expediente_nuc']}\n"
                        f"**Relevancia:** {f['score']:.3f}\n"
                        f"**Contenido:** {f['contenido']}\n"
                        f"{'='*50}"
                        for i, f in enumerate(fuentes_formateadas[:5])
                    ])
                    
                    prompt = f"""Eres un experto jurista especializado en derechos humanos y derecho penal internacional. 

Responde la siguiente consulta sobre el genocidio de la Uni√≥n Patri√≥tica bas√°ndote EXCLUSIVAMENTE en la informaci√≥n de los documentos judiciales proporcionados.

**Consulta:** {pregunta}

**Contexto de documentos judiciales:**
{contexto_chunks}

INSTRUCCIONES IMPORTANTES:
1. Proporciona una respuesta detallada y jur√≠dicamente fundamentada
2. INCLUYE CITAS ESPEC√çFICAS en el texto usando el formato [Doc 1], [Doc 2], etc.
3. Cada afirmaci√≥n debe estar respaldada por referencias a los documentos
4. Incluye los elementos constitutivos del genocidio seg√∫n el derecho internacional
5. Explica c√≥mo se configuran estos elementos en el caso de la Uni√≥n Patri√≥tica
6. Menciona reconocimiento jurisprudencial cuando est√© en los documentos
7. La respuesta debe ser completa (m√≠nimo 800 palabras) y bien estructurada

FORMATO DE CITAS: Usa [Doc 1], [Doc 2], [Doc 3] para referenciar los documentos proporcionados.

Respuesta:"""

                    response = azure_client.chat.completions.create(
                        model=os.getenv('AZURE_OPENAI_DEPLOYMENT', 'gpt-4o-mini'),
                        messages=[{"role": "user", "content": prompt}],
                        max_tokens=2500,  # Aumentado para respuestas m√°s completas
                        temperature=0.3
                    )
                    
                    respuesta_texto = response.choices[0].message.content
                    
                    return {
                        'respuesta': respuesta_texto,
                        'fuentes': fuentes_formateadas,
                        'confianza': 0.9,
                        'metodo': 'azure_search_rag',
                        'tiempo_respuesta': 0,
                        'num_fuentes': len(fuentes_formateadas)
                    }
                else:
                    return {
                        'respuesta': "No se encontraron documentos relevantes en Azure Search para esta consulta.",
                        'fuentes': [],
                        'confianza': 0.1,
                        'metodo': 'azure_search_fallback'
                    }
                
            except Exception as e:
                logging.error(f"Error en Azure Search RAG: {str(e)}")
                return {
                    'respuesta': f"Error en consulta Azure Search: {str(e)}",
                    'fuentes': [],
                    'confianza': 0.0,
                    'metodo': 'error'
                }
        
        # Ejecutar la funci√≥n async
        try:
            resultado = loop.run_until_complete(_ejecutar_consulta())
            return resultado
        finally:
            loop.close()
            
    except Exception as e:
        logging.error(f"Error en consulta_hibrida_sincrona: {str(e)}")
        return {
            'respuesta': f"Error general en consulta: {str(e)}",
            'fuentes': [],
            'confianza': 0.0,
            'metodo': 'error_general'
        }
        
        # Ejecutar la consulta as√≠ncrona
        resultado = asyncio.run(_ejecutar_consulta())
        return resultado
        
    except Exception as e:
        return {
            'respuesta': f"Error en consulta RAG con Azure Search: {str(e)}",
            'fuentes': [],
            'confianza': 0.0,
            'metodo': 'error',
            'tiempo_respuesta': 0
        }

async def main():
    """Ejemplo de uso del sistema RAG"""
    sistema = SistemaRAGCompleto()
    
    # Configurar logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # Ejemplo de consulta compleja
    consulta = ConsultaRAG(
        pregunta="¬øQu√© es el genocidio de la Uni√≥n Patri√≥tica?",
        tipo_consulta=TipoConsulta.COMPLEJA,
        contexto_adicional={"campo_interes": "analisis_juridico"}
    )
    
    respuesta, consulta_id = await sistema.procesar_consulta(consulta)
    
    print(f"Consulta ID: {consulta_id}")
    print(f"Respuesta: {respuesta.texto}")
    print(f"Confianza: {respuesta.confianza}")
    print(f"M√©todo: {respuesta.metodo.value}")
    print(f"Tiempo: {respuesta.tiempo_respuesta}ms")
    print(f"Fuentes: {len(respuesta.fuentes)}")
    
    # Ejemplo de feedback
    feedback = FeedbackRAG(
        calificacion=5,
        comentario="Excelente respuesta, muy completa",
        aspectos={"precision": 5, "relevancia": 5, "completitud": 4}
    )
    
    await sistema.registrar_feedback(consulta_id, 1, feedback)
    print("\nFeedback registrado")

class SistemaRAGCompleto:
    """Clase wrapper para mantener compatibilidad con la interfaz principal"""
    
    def __init__(self):
        """Inicializar el sistema RAG completo"""
        self.azure_search = None
        try:
            self.azure_search = AzureSearchVectorizado()
        except Exception as e:
            print(f"Warning: Azure Search no disponible: {e}")
    
    def consulta_hibrida(self, pregunta: str) -> Dict[str, Any]:
        """Ejecutar consulta h√≠brida usando la funci√≥n s√≠ncrona"""
        try:
            return consulta_hibrida_sincrona(pregunta)
        except Exception as e:
            return {
                'respuesta': f"Error en consulta h√≠brida: {str(e)}",
                'fuentes': [],
                'confianza': 0.0,
                'metodos_usados': ['error']
            }
    
    async def busqueda_cruzada_avanzada(self, pregunta: str, filtros_documento: Optional[Dict] = None, 
                                        filtros_chunk: Optional[Dict] = None, 
                                        filtros_azure_search: Optional[Dict] = None) -> Dict[str, Any]:
        """
        Realiza b√∫squeda cruzada entre documentos completos y chunks para mejor filtrado
        
        Returns:
            Dict con respuesta, fuentes_documentos, fuentes_chunks, metadatos_filtrado
        """
        if not self.azure_search:
            return {
                'respuesta': "Azure Search no est√° disponible",
                'fuentes_documentos': [],
                'fuentes_chunks': [],
                'metadatos_filtrado': {},
                'confianza': 0.0
            }
        
        try:
            # Procesar filtros universales si est√°n disponibles
            filtros_docs_final = filtros_documento
            filtros_chunks_final = filtros_chunk
            
            if filtros_azure_search:
                # Usar filtros del sistema universal
                filtros_docs_final = filtros_azure_search.get('documentos')
                filtros_chunks_final = filtros_azure_search.get('chunks')
            
            # Realizar b√∫squeda cruzada con filtros aplicados
            documentos, chunks = await self.azure_search.busqueda_cruzada(
                pregunta, filtros_docs_final, filtros_chunks_final, top_k_docs=20, top_k_chunks=5
            )
            
            # Obtener metadatos para filtrado din√°mico
            metadatos_filtrado = self.azure_search.obtener_metadatos_filtrado(documentos)
            
            # Formatear documentos completos
            fuentes_documentos = []
            for doc in documentos:
                fuentes_documentos.append({
                    'id': doc.id,
                    'expediente_nuc': doc.expediente_nuc,
                    'tipo_documental': doc.tipo_documental,
                    'nombre_archivo': doc.nombre_archivo,
                    'fecha_documento': doc.fecha_documento,
                    'departamento': doc.departamento,
                    'municipio': doc.municipio,
                    'organizacion_responsable': doc.organizacion_responsable,
                    'contenido': doc.contenido_completo[:500] + '...' if len(doc.contenido_completo) > 500 else doc.contenido_completo,
                    'score': doc.score
                })
            
            # Formatear chunks (reutilizar l√≥gica existente)
            fuentes_chunks = []
            if chunks:
                for i, chunk in enumerate(chunks, 1):
                    try:
                        nombre_archivo = chunk.nombre_archivo if hasattr(chunk, 'nombre_archivo') else 'N/A'
                        if nombre_archivo.endswith('.json'):
                            partes = nombre_archivo.replace('.json', '').split('_')
                            if len(partes) >= 3:
                                fecha = partes[0] if partes[0].isdigit() else 'S/F'
                                tipo_doc = partes[1] if len(partes) > 1 else 'DOC'
                                nombre_archivo = f"{tipo_doc}_{fecha}"
                        
                        contenido = chunk.contenido if hasattr(chunk, 'contenido') else ''
                        contenido_limpio = contenido.replace('\\n', ' ').replace('\\t', ' ')
                        
                        pagina = chunk.metadata.get('pagina', 'N/A') if hasattr(chunk, 'metadata') and chunk.metadata else 'N/A'
                        parrafo = chunk.metadata.get('parrafo', 'N/A') if hasattr(chunk, 'metadata') and chunk.metadata else 'N/A'
                        
                        fuentes_chunks.append({
                            'archivo': nombre_archivo,
                            'contenido': contenido_limpio[:500] + '...' if len(contenido_limpio) > 500 else contenido_limpio,
                            'score': float(chunk.score if hasattr(chunk, 'score') else 0.0),
                            'pagina': pagina,
                            'parrafo': parrafo,
                            'tipo_documental': chunk.tipo_documental if hasattr(chunk, 'tipo_documental') else 'Documento Judicial',
                            'expediente_nuc': chunk.expediente_nuc if hasattr(chunk, 'expediente_nuc') else 'N/A',
                            'metadata': chunk.metadata if hasattr(chunk, 'metadata') else {},
                            'doc_ref': f"Chunk {i}"
                        })
                    except Exception as e:
                        logging.error(f"Error procesando chunk {i}: {e}")
            
            # Generar respuesta con contexto combinado
            contexto_combinado = ""
            if documentos:
                contexto_combinado += "DOCUMENTOS COMPLETOS RELEVANTES:\n"
                for i, doc in enumerate(documentos[:3], 1):
                    contexto_combinado += f"\nDocumento {i}:\n"
                    contexto_combinado += f"Expediente: {doc.expediente_nuc}\n"
                    contexto_combinado += f"Tipo: {doc.tipo_documental}\n"
                    contexto_combinado += f"Ubicaci√≥n: {doc.departamento}, {doc.municipio}\n"
                    contexto_combinado += f"Organizaci√≥n: {doc.organizacion_responsable}\n"
                    contexto_combinado += f"Contenido: {doc.contenido_completo[:800]}\n"
                    contexto_combinado += "-" * 80 + "\n"
            
            if chunks:
                contexto_combinado += "\nCHUNKS ESPEC√çFICOS RELEVANTES:\n"
                for i, chunk in enumerate(chunks, 1):
                    contexto_combinado += f"\nChunk {i}:\n"
                    contexto_combinado += f"Expediente: {chunk.expediente_nuc}\n"
                    contexto_combinado += f"Tipo: {chunk.tipo_documental}\n"
                    contexto_combinado += f"Contenido: {chunk.contenido}\n"
                    contexto_combinado += "-" * 80 + "\n"
            
            # Generar respuesta usando Azure OpenAI
            if self.azure_search.openai_client and contexto_combinado:
                prompt = f"""
Eres un experto analista de documentos jur√≠dicos especializizado en cr√≠menes de lesa humanidad.

CONTEXTO DOCUMENTAL:
{contexto_combinado}

PREGUNTA DEL USUARIO: {pregunta}

Instrucciones:
1. Analiza la informaci√≥n de los documentos completos Y los chunks espec√≠ficos
2. Proporciona una respuesta integral y detallada
3. Incluye citas espec√≠ficas a expedientes, tipos documentales y organizaciones
4. Menciona ubicaciones geogr√°ficas cuando sea relevante
5. Estructura tu respuesta de manera clara y profesional
6. Si hay informaci√≥n contradictoria, mencionala
7. Usa un m√°ximo de 2500 tokens para la respuesta

RESPUESTA:
"""
                try:
                    response = await self.azure_search.openai_client.chat.completions.acreate(
                        model="gpt-4",
                        messages=[{"role": "user", "content": prompt}],
                        max_tokens=2500,
                        temperature=0.7
                    )
                    respuesta = response.choices[0].message.content
                except Exception as e:
                    respuesta = f"Error generando respuesta con IA: {str(e)}. Contexto disponible con {len(documentos)} documentos y {len(chunks)} chunks."
            else:
                respuesta = f"Informaci√≥n encontrada: {len(documentos)} documentos completos y {len(chunks)} chunks espec√≠ficos. Contexto disponible pero sin generaci√≥n IA."
            
            return {
                'respuesta': respuesta,
                'fuentes_documentos': fuentes_documentos,
                'fuentes_chunks': fuentes_chunks,
                'metadatos_filtrado': metadatos_filtrado,
                'confianza': 0.8 if documentos and chunks else 0.6,
                'estadisticas': {
                    'documentos_encontrados': len(documentos),
                    'chunks_encontrados': len(chunks),
                    'filtros_aplicados_docs': filtros_documento or {},
                    'filtros_aplicados_chunks': filtros_chunk or {}
                }
            }
            
        except Exception as e:
            logging.error(f"Error en b√∫squeda cruzada avanzada: {e}")
            return {
                'respuesta': f"Error en b√∫squeda cruzada: {str(e)}",
                'fuentes_documentos': [],
                'fuentes_chunks': [],
                'metadatos_filtrado': {},
                'confianza': 0.0
            }

if __name__ == "__main__":
    asyncio.run(main())
